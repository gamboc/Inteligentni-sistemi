{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpQeEGjo5c2jGiAkC0y++7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Preprocessing"],"metadata":{"id":"wgi6mID1qN4o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GsEQv3itqMtH"},"outputs":[],"source":["import json\n","import validators\n","import urllib.parse\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem import PorterStemmer\n","\n","\n","# reduces the link to the webside nime, for example \"https://www.google.com/search?q=fri\" returns just \"google\"\n","def find_source(link):\n","    url = urllib.parse.urlparse(link).netloc\n","    url = '.'.join(url.split('.')[-2:])\n","\n","    url = url.split('.')\n","    return url[0]\n","\n","# adds all json entries to a single array in the form of a dictionary (except those with no headline or description)\n","articles = []\n","with open(\"News_Category_Dataset_IS_course.json\", \"r\") as file:\n","    for line in file:\n","        j = json.loads(line)\n","        if j['headline'] is None or j['short_description'] is None:\n","            continue\n","        articles.append(j)\n","\n","for article in articles:\n","    # solves the problem of missing author\n","    if len(article['authors']) < 1:\n","        article['authors'] = \"Jane Doe\"\n","\n","    # solves the problem of invalid links\n","    if not validators.url(article['link']):\n","        article['link'] = article['link'][30:]\n","\n","    source = find_source(article['link'])\n","    if source == \"huffpost\":\n","        article['source'] = \"huffingtonpost\"\n","    else:\n","        article['source'] = source\n","    article.pop('link')\n","\n","# reduces the headlines and descriptions into base forms of words and removes stopwords\n","for article in articles:\n","    lemmatizer = WordNetLemmatizer()\n","    stemmer = PorterStemmer()\n","\n","    headline = article['headline']\n","    desc = article['short_description']\n","\n","    headline = word_tokenize(headline.lower())\n","    desc = word_tokenize(desc.lower())\n","    article['authors'] = article['authors'].lower()\n","\n","    table = str.maketrans('', '', string.punctuation)\n","    headline = [word.translate(table) for word in headline if word.isalpha()]\n","    desc = [word.translate(table) for word in desc if word.isalpha()]\n","\n","    stop_words = set(stopwords.words('english'))\n","    headline = [word for word in headline if word not in stop_words]\n","    desc = [word for word in desc if word not in stop_words]\n","\n","    headline = [lemmatizer.lemmatize(word) for word in headline]\n","    desc = [lemmatizer.lemmatize(word) for word in desc]\n","\n","    headline = [stemmer.stem(word) for word in headline]\n","    desc = [stemmer.stem(word) for word in desc]\n","\n","    article['headline'] = ' '.join(headline)\n","    article['short_description'] = ' '.join(desc)\n","\n","# saves the edited data\n","file = open(\"preprocessed.json\", \"a\")\n","for article in articles:\n","    file.write(json.dumps(article))\n","    file.write(\"\\n\")"]},{"cell_type":"markdown","source":["#Decision tree"],"metadata":{"id":"3uG2LkTSqXIR"}},{"cell_type":"code","source":["import json\n","import sys\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import classification_report\n","\n","# loads the preprocessed data\n","articles = []\n","with open(\"preprocessed.json\", \"r\") as file:\n","    for line in file:\n","        j = json.loads(line)\n","        articles.append(j)\n","\n","# joins the separate values into a single string for every article and vectorizes the data\n","X = []\n","Y = []\n","for article in articles:\n","    X.append(article['source'] + ' ' + article['headline'] + ' ' + article['authors'] + ' ' + article['short_description'] + ' ' + str(article['date']))\n","    Y.append(article['category'])\n","\n","vectorizer = TfidfVectorizer()\n","\n","X_vector = vectorizer.fit_transform(X)\n","# splits the data for training and testing\n","X_train, X_test, Y_train, Y_test = train_test_split(X_vector, Y, test_size=0.2, random_state=42)\n","\n","# trains the model\n","decision_tree_classifier = DecisionTreeClassifier()\n","decision_tree_classifier.fit(X_train, Y_train)\n","print(decision_tree_classifier.get_depth())\n","\n","# tests the model\n","Y_predictions = decision_tree_classifier.predict(X_test)\n","\n","# prints the report to a file\n","file = open(\"./results/tree\", \"w\")\n","sys.stdout = file\n","report = classification_report(Y_test, Y_predictions)\n","print(report)\n","sys.stdout = sys.__stdout__\n","file.close()"],"metadata":{"id":"A-P39dq7qYsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#k-nearest neighbors"],"metadata":{"id":"o1g0xitRqsSK"}},{"cell_type":"code","source":["import json\n","import sys\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report\n","\n","# loads the preprocessed data\n","articles = []\n","with open(\"preprocessed.json\", \"r\") as file:\n","    for line in file:\n","        j = json.loads(line)\n","        articles.append(j)\n","\n","# joins the separate values into a single string for every article and vectorizes the data\n","X = []\n","Y = []\n","for article in articles:\n","    X.append(article['source'] + ' ' + article['headline'] + ' ' + article['authors'] + ' ' + article['short_description'] + ' ' + str(article['date']))\n","    Y.append(article['category'])\n","\n","vectorizer = TfidfVectorizer()\n","\n","X_vector = vectorizer.fit_transform(X)\n","\n","# splits the data for training and testing\n","X_train, X_test, Y_train, Y_test = train_test_split(X_vector, Y, test_size=0.2, random_state=42)\n","\n","# trains different knn models, changing the hyperparameter k each time\n","for i in [3, 4, 5, 6, 7]:\n","    knn_classifier = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n","    knn_classifier.fit(X_train, Y_train)\n","\n","    file = open(\"./results/knn_result\" + \"_\" + str(i), \"a\")\n","    sys.stdout = file\n","\n","    Y_predictions = knn_classifier.predict(X_test)\n","\n","    # writes the report of each iteration on a separate file\n","    report = classification_report(Y_test, Y_predictions)\n","    print(report)\n","\n","    sys.stdout = sys.__stdout__\n","    file.close()"],"metadata":{"id":"4CqL8rZwqu-B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Random forest"],"metadata":{"id":"tfPJd8xLrNO7"}},{"cell_type":"code","source":["import json\n","import sys\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import classification_report\n","\n","# loads the preprocessed data\n","articles = []\n","with open(\"preprocessed.json\", \"r\") as file:\n","    for line in file:\n","        j = json.loads(line)\n","        articles.append(j)\n","\n","# joins the separate values into a single string for every article\n","X = []\n","Y = []\n","for article in articles:\n","    X.append(article['source'] + ' ' + article['headline'] + ' ' + article['authors'] + ' ' + article['short_description'] + ' ' + str(article['date']))\n","    Y.append(article['category'])\n","\n","# splits the data for training and testing\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# sets the parameters for grid search\n","grid = {\n","    'randomforestclassifier__max_features': ['sqrt', 'log2', None],\n","    'randomforestclassifier__n_estimators': [50, 75, 100],\n","    'randomforestclassifier__max_depth': [200, 400, None]\n","}\n","\n","# creates the pipeline and trains the model\n","pipeline = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n","grid_search = GridSearchCV(pipeline, grid, cv=3, n_jobs=-1)\n","\n","grid_search.fit(X_train, Y_train)\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","Y_predictions = best_model.predict(X_test)\n","\n","# prints the repost to a separate file\n","file = open(\"./results/forest\", \"a\")\n","sys.stdout = file\n","report = classification_report(Y_test, Y_predictions)\n","print(report)\n","sys.stdout = sys.__stdout__\n","file.close()\n","\n","# prints the parameters of the best model to a separate file\n","file = open(\"./results/forest_params\", \"w\")\n","sys.stdout = file\n","print(best_params)\n","sys.stdout = sys.__stdout__\n","file.close()"],"metadata":{"id":"xYWz547WrOdk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#XGBoost"],"metadata":{"id":"OfrmyNQTr8nB"}},{"cell_type":"code","source":["import json\n","import xgboost as xbg\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import sys\n","\n","# loads the preprocessed data\n","articles = []\n","with open(\"preprocessed.json\", \"r\") as file:\n","    for line in file:\n","        j = json.loads(line)\n","        articles.append(j)\n","\n","# joins the separate values into a single string for every article\n","X = []\n","Y = []\n","for article in articles:\n","    X.append(article['source'] + ' ' + article['headline'] + ' ' + article['authors'] + ' ' + article['short_description'] + ' ' + str(article['date']))\n","    Y.append(article['category'])\n","\n","vectorizer = TfidfVectorizer()\n","encoder = LabelEncoder()\n","\n","# vectorizes the data, encodes the array of categories and splits the data into training and testing\n","X_vector = vectorizer.fit_transform(X)\n","Y_encoded = encoder.fit_transform(Y)\n","X_train, X_test, Y_train, Y_test = train_test_split(X_vector, Y_encoded, test_size=0.2, random_state=42)\n","\n","# trains the model\n","xgboost_classifier = xbg.XGBClassifier(n_jobs=-1)\n","xgboost_classifier.fit(X_train, Y_train)\n","\n","# decodes the predictions\n","Y_predictions = xgboost_classifier.predict(X_test)\n","Y_predictions = encoder.inverse_transform(Y_predictions)\n","Y_test = encoder.inverse_transform(Y_test)\n","\n","# prints the results to a separate file\n","file = open(\"./results/xgboost\", \"a\")\n","sys.stdout = file\n","report = classification_report(Y_test, Y_predictions)\n","print(report)\n","sys.stdout = sys.__stdout__\n","file.close()"],"metadata":{"id":"TSyV0HR3r-Oz"},"execution_count":null,"outputs":[]}]}